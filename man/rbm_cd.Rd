% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rbm.R
\name{rbm_cdk}
\alias{rbm_cdk}
\alias{rbm_ucd}
\title{Train RBM Models}
\usage{
rbm_cdk(
  m,
  n,
  dat,
  b0 = NULL,
  c0 = NULL,
  w0 = NULL,
  batch_size = 10L,
  lr = 0.1,
  momentum = 0,
  niter = 100L,
  ngibbs = 10L,
  nchain = 1L,
  persistent = FALSE,
  eval_loglik = FALSE,
  exact_loglik = FALSE,
  eval_freq = 10L,
  eval_size = 100L,
  eval_nmc = 100L,
  eval_nstep = 10L,
  nthread = 1L,
  verbose = 0L
)

rbm_ucd(
  m,
  n,
  dat,
  b0 = NULL,
  c0 = NULL,
  w0 = NULL,
  batch_size = 10L,
  lr = 0.1,
  momentum = 0,
  niter = 100L,
  min_mcmc = 1L,
  max_mcmc = 100L,
  nchain = 1L,
  eval_loglik = FALSE,
  exact_loglik = FALSE,
  eval_freq = 10L,
  eval_size = 100L,
  eval_nmc = 100L,
  eval_nstep = 10L,
  nthread = 1L,
  verbose = 0L
)
}
\arguments{
\item{m}{Dimension of the visible units.}

\item{n}{Dimension of the hidden units.}

\item{dat}{The observed data, of size \code{[m x N]}.}

\item{b0}{Initial value for the bias parameter for the visible units, of size \code{[m x 1]}.}

\item{c0}{Initial value for the bias parameter for the hidden units, of size \code{[n x 1]}.}

\item{w0}{Initial value for the weight parameter of the RBM, of size \code{[m x n]}.}

\item{batch_size}{Size of the mini-batch.}

\item{lr}{Learning rate.}

\item{momentum}{Momentum coefficient in SGD.}

\item{niter}{Number of iterations.}

\item{ngibbs}{The "k" in the CD-k algorithm.}

\item{nchain}{Number of independent Markov chains to approximate gradient.}

\item{persistent}{Whether to use PCD instead of CD.}

\item{eval_loglik}{Whether to compute log-likelihood values during training.}

\item{exact_loglik}{Compute exact or approximate log-likelihood values.}

\item{eval_freq}{Evaluate log-likelihood every \code{eval_freq} mini-batches.}

\item{eval_size}{Size of sub-sampled data to evaluate log-likelihood.}

\item{eval_nmc}{Size of the Monte Carlo sample for approximating the log-likelihood.}

\item{eval_nstep}{Number of steps in the Gibbs sampler for approximating the log-likelihood.}

\item{nthread}{Number of threads for parallel computing, if OpenMP is supported.}

\item{verbose}{Level of verbosity.}
}
\description{
\code{rbm_cdk()} uses the traditional CD-k algorithm to train RBM.
\code{rbm_ucd()} uses the unbiased CD algorithm.
}
\examples{
\dontrun{
# Bars-and-stripes data, Schulz et al. (2010), Fischer and Igel (2010)
d = 4
n = m = d^2
N = 2^d
dat = matrix(0, 2 * N, m)

for(i in 1:N)
{
    bits = as.integer(rev(intToBits(i - 1)[1:d]))
    mat = tcrossprod(bits, rep(1, d))
    dat[2 * i - 1, ] = as.numeric(mat)
    dat[2 * i, ] = as.numeric(t(mat))
}

N = nrow(dat)

# Persistent contrastive divergence
set.seed(123)
pcd = rbm_cdk(m, n, t(dat), batch_size = N, lr = 0.1, niter = 1000,
              ngibbs = 1, nchain = 1000, persistent = TRUE,
              eval_loglik = TRUE, exact_loglik = TRUE,
              eval_freq = 1, eval_size = N, verbose = 1)

# Unbiased contrastive divergence
set.seed(123)
ucd = rbm_ucd(m, n, t(dat), batch_size = N, lr = 0.1, niter = 1000,
              min_mcmc = 1, max_mcmc = 100, nchain = 1000,
              eval_loglik = TRUE, exact_loglik = TRUE,
              eval_freq = 1, eval_size = N, verbose = 1)

plot(pcd$loglik, type = "l")
lines(ucd$loglik, col = "blue")
}

}
